#!/usr/bin/env python3
"""
Performance optimization script for RareSift
Analyzes and implements performance improvements
"""

import os
import re
import json
import subprocess
import time
from pathlib import Path
from typing import List, Dict, Tuple
import shutil

class RareSiftOptimizer:
    def __init__(self):
        self.project_root = Path.cwd()
        self.optimizations_applied = []
        self.recommendations = []
        
    def check_database_indexes(self) -> List[str]:
        """Check for missing database indexes"""
        print("üóÑÔ∏è Analyzing database performance...")
        
        recommendations = []
        
        # Check models for frequently queried fields without indexes
        models_dir = self.project_root / "backend" / "app" / "models"
        if models_dir.exists():
            for model_file in models_dir.glob("*.py"):
                content = model_file.read_text()
                
                # Look for common patterns that need indexes
                patterns = [
                    (r'user_id.*=.*Column', 'user_id field should have index for faster user queries'),
                    (r'video_id.*=.*Column', 'video_id field should have index for faster video queries'),
                    (r'created_at.*=.*Column', 'created_at field should have index for time-based queries'),
                    (r'status.*=.*Column', 'status field should have index for filtering'),
                ]
                
                for pattern, suggestion in patterns:
                    if re.search(pattern, content) and 'index=True' not in content:
                        recommendations.append(f"{model_file.name}: {suggestion}")
        
        return recommendations
    
    def optimize_database_queries(self) -> List[str]:
        """Optimize database queries"""
        print("üîç Optimizing database queries...")
        
        optimizations = []
        
        # Check API endpoints for N+1 queries and missing eager loading
        api_dir = self.project_root / "backend" / "app" / "api"
        if api_dir.exists():
            for api_file in api_dir.rglob("*.py"):
                content = api_file.read_text()
                
                # Look for potential N+1 query patterns
                if '.all()' in content and 'joinedload' not in content:
                    recommendations = f"Consider using joinedload() in {api_file.name} to prevent N+1 queries"
                    self.recommendations.append(recommendations)
                
                # Check for missing pagination
                if '.all()' in content and 'limit' not in content.lower():
                    recommendations = f"Add pagination to {api_file.name} to limit query results"
                    self.recommendations.append(recommendations)
        
        return optimizations
    
    def optimize_docker_configuration(self) -> List[str]:
        """Optimize Docker configurations for performance"""
        print("üê≥ Optimizing Docker configurations...")
        
        optimizations = []
        
        # Check production Dockerfiles
        dockerfiles = [
            self.project_root / "backend" / "Dockerfile.prod",
            self.project_root / "frontend" / "Dockerfile.prod"
        ]
        
        for dockerfile in dockerfiles:
            if dockerfile.exists():
                content = dockerfile.read_text()
                
                # Check for multi-stage builds
                if 'FROM' not in content or content.count('FROM') < 2:
                    self.recommendations.append(f"Use multi-stage builds in {dockerfile.name} to reduce image size")
                
                # Check for layer optimization
                if 'RUN apt-get update && apt-get install' not in content:
                    if 'RUN apt-get update' in content and 'RUN apt-get install' in content:
                        self.recommendations.append(f"Combine apt-get commands in {dockerfile.name} to reduce layers")
                
                # Check for proper caching
                if 'COPY requirements.txt' not in content and 'COPY package.json' not in content:
                    self.recommendations.append(f"Copy dependency files first in {dockerfile.name} for better caching")
        
        return optimizations
    
    def optimize_frontend_performance(self) -> List[str]:
        """Optimize frontend performance"""
        print("‚öõÔ∏è Optimizing frontend performance...")
        
        optimizations = []
        
        # Check Next.js configuration
        next_config = self.project_root / "frontend" / "next.config.js"
        if next_config.exists():
            content = next_config.read_text()
            
            improvements = {
                'swcMinify: true': 'Enable SWC minification for faster builds',
                'compress: true': 'Enable gzip compression',
                'poweredByHeader: false': 'Remove X-Powered-By header for security',
                'generateEtags: false': 'Disable ETags for better caching control'
            }
            
            for config, description in improvements.items():
                if config not in content:
                    self.recommendations.append(f"Add {config} to next.config.js: {description}")
        
        # Check for image optimization
        images_dir = self.project_root / "frontend" / "public"
        if images_dir.exists():
            large_images = []
            for img_file in images_dir.rglob("*.{jpg,jpeg,png,gif}"):
                if img_file.stat().st_size > 500 * 1024:  # > 500KB
                    large_images.append(img_file.name)
            
            if large_images:
                self.recommendations.append(f"Optimize large images: {', '.join(large_images[:5])}")
        
        return optimizations
    
    def optimize_backend_performance(self) -> List[str]:
        """Optimize backend performance"""
        print("üöÄ Optimizing backend performance...")
        
        optimizations = []
        
        # Check for missing async/await patterns
        api_files = list((self.project_root / "backend" / "app" / "api").rglob("*.py"))
        for api_file in api_files:
            content = api_file.read_text()
            
            # Check for synchronous database calls
            if 'session.query' in content or 'session.get' in content:
                if 'async def' not in content:
                    self.recommendations.append(f"Consider using async/await in {api_file.name} for better concurrency")
        
        # Check requirements.txt for performance packages
        requirements_file = self.project_root / "backend" / "requirements.txt"
        if requirements_file.exists():
            content = requirements_file.read_text()
            
            performance_packages = {
                'redis': 'Add Redis for caching',
                'asyncpg': 'Use asyncpg for better PostgreSQL async performance',
                'uvloop': 'Use uvloop for faster event loop',
                'orjson': 'Use orjson for faster JSON serialization'
            }
            
            for package, description in performance_packages.items():
                if package not in content:
                    self.recommendations.append(f"Consider adding {package}: {description}")
        
        return optimizations
    
    def create_caching_configuration(self) -> str:
        """Create Redis caching configuration"""
        print("üíæ Creating caching configuration...")
        
        cache_config = '''"""
Redis caching configuration for RareSift
"""

import redis
from typing import Optional, Any
import json
import pickle
from app.core.config import settings

class CacheManager:
    def __init__(self):
        self.redis_client = redis.Redis(
            host=settings.redis_host,
            port=settings.redis_port,
            db=settings.redis_db,
            decode_responses=False
        )
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        try:
            data = self.redis_client.get(key)
            if data:
                return pickle.loads(data)
            return None
        except Exception:
            return None
    
    def set(self, key: str, value: Any, ttl: int = 3600) -> bool:
        """Set value in cache with TTL"""
        try:
            serialized = pickle.dumps(value)
            return self.redis_client.setex(key, ttl, serialized)
        except Exception:
            return False
    
    def delete(self, key: str) -> bool:
        """Delete key from cache"""
        try:
            return bool(self.redis_client.delete(key))
        except Exception:
            return False
    
    def get_search_results(self, query_hash: str) -> Optional[dict]:
        """Get cached search results"""
        return self.get(f"search:{query_hash}")
    
    def cache_search_results(self, query_hash: str, results: dict, ttl: int = 1800):
        """Cache search results for 30 minutes"""
        return self.set(f"search:{query_hash}", results, ttl)
    
    def get_video_metadata(self, video_id: str) -> Optional[dict]:
        """Get cached video metadata"""
        return self.get(f"video_meta:{video_id}")
    
    def cache_video_metadata(self, video_id: str, metadata: dict, ttl: int = 3600):
        """Cache video metadata for 1 hour"""
        return self.set(f"video_meta:{video_id}", metadata, ttl)

# Global cache instance
cache = CacheManager()
'''
        
        cache_file = self.project_root / "backend" / "app" / "core" / "cache.py"
        if not cache_file.exists():
            cache_file.write_text(cache_config)
            self.optimizations_applied.append(f"Created caching configuration: {cache_file}")
            return str(cache_file)
        
        return ""
    
    def create_performance_middleware(self) -> str:
        """Create performance monitoring middleware"""
        print("üìä Creating performance monitoring middleware...")
        
        middleware_config = '''"""
Performance monitoring middleware for RareSift
"""

import time
import logging
from typing import Callable
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware

logger = logging.getLogger(__name__)

class PerformanceMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, slow_request_threshold: float = 1.0):
        super().__init__(app)
        self.slow_request_threshold = slow_request_threshold
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        start_time = time.time()
        
        # Add request ID for tracing
        request_id = f"{int(start_time * 1000000)}"
        
        response = await call_next(request)
        
        process_time = time.time() - start_time
        response.headers["X-Process-Time"] = str(process_time)
        response.headers["X-Request-ID"] = request_id
        
        # Log slow requests
        if process_time > self.slow_request_threshold:
            logger.warning(
                f"Slow request: {request.method} {request.url.path} "
                f"took {process_time:.3f}s (threshold: {self.slow_request_threshold}s)"
            )
        
        # Log request metrics
        logger.info(
            f"{request.method} {request.url.path} - "
            f"{response.status_code} - {process_time:.3f}s"
        )
        
        return response

class DatabaseQueryMiddleware(BaseHTTPMiddleware):
    """Middleware to track database query performance"""
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        # This would integrate with SQLAlchemy to track query counts and times
        # Implementation depends on your database setup
        
        query_start = time.time()
        response = await call_next(request)
        query_time = time.time() - query_start
        
        # Add query performance headers (for development)
        if hasattr(request.state, 'query_count'):
            response.headers["X-DB-Query-Count"] = str(request.state.query_count)
            response.headers["X-DB-Query-Time"] = str(query_time)
        
        return response
'''
        
        middleware_file = self.project_root / "backend" / "app" / "core" / "performance_middleware.py"
        if not middleware_file.exists():
            middleware_file.write_text(middleware_config)
            self.optimizations_applied.append(f"Created performance middleware: {middleware_file}")
            return str(middleware_file)
        
        return ""
    
    def optimize_nginx_configuration(self) -> List[str]:
        """Optimize nginx configuration for performance"""
        print("üåê Optimizing nginx configuration...")
        
        optimizations = []
        nginx_config = self.project_root / "nginx" / "nginx.conf"
        
        if nginx_config.exists():
            content = nginx_config.read_text()
            
            # Check for gzip compression
            if 'gzip on' not in content:
                self.recommendations.append("Enable gzip compression in nginx.conf")
            
            # Check for proper caching headers
            if 'expires' not in content:
                self.recommendations.append("Add expires headers for static assets in nginx.conf")
            
            # Check for connection optimization
            if 'keepalive_timeout' not in content:
                self.recommendations.append("Configure keepalive_timeout in nginx.conf")
            
            # Check for worker optimization
            if 'worker_processes auto' not in content:
                self.recommendations.append("Set worker_processes to auto in nginx.conf")
        
        return optimizations
    
    def create_database_optimization_script(self) -> str:
        """Create database optimization script"""
        print("üóÑÔ∏è Creating database optimization script...")
        
        db_optimization_script = '''#!/usr/bin/env python3
"""
Database optimization script for RareSift
Creates indexes and optimizes database performance
"""

from sqlalchemy import text
from app.core.database import engine

def create_performance_indexes():
    """Create indexes for better performance"""
    
    indexes = [
        # Video table indexes
        "CREATE INDEX IF NOT EXISTS idx_videos_user_id ON videos(user_id);",
        "CREATE INDEX IF NOT EXISTS idx_videos_created_at ON videos(created_at);",
        "CREATE INDEX IF NOT EXISTS idx_videos_status ON videos(status);",
        "CREATE INDEX IF NOT EXISTS idx_videos_title_gin ON videos USING gin(to_tsvector('english', title));",
        
        # Frame table indexes
        "CREATE INDEX IF NOT EXISTS idx_frames_video_id ON frames(video_id);",
        "CREATE INDEX IF NOT EXISTS idx_frames_timestamp ON frames(timestamp);",
        
        # Embedding table indexes
        "CREATE INDEX IF NOT EXISTS idx_embeddings_frame_id ON embeddings(frame_id);",
        
        # Search table indexes
        "CREATE INDEX IF NOT EXISTS idx_searches_user_id ON searches(user_id);",
        "CREATE INDEX IF NOT EXISTS idx_searches_created_at ON searches(created_at);",
        
        # User table indexes
        "CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);",
        "CREATE INDEX IF NOT EXISTS idx_users_created_at ON users(created_at);"
    ]
    
    with engine.connect() as conn:
        for index_sql in indexes:
            try:
                conn.execute(text(index_sql))
                print(f"‚úÖ Created index: {index_sql.split()[5]}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Index might already exist: {e}")
        
        conn.commit()

def analyze_database_performance():
    """Analyze database performance"""
    
    analysis_queries = [
        # Check table sizes
        """
        SELECT schemaname, tablename, 
               pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
        FROM pg_tables 
        WHERE schemaname = 'public'
        ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
        """,
        
        # Check index usage
        """
        SELECT schemaname, tablename, indexname, idx_tup_read, idx_tup_fetch
        FROM pg_stat_user_indexes
        ORDER BY idx_tup_read DESC;
        """,
        
        # Check slow queries (if pg_stat_statements is enabled)
        """
        SELECT query, calls, total_time, mean_time
        FROM pg_stat_statements
        ORDER BY mean_time DESC
        LIMIT 10;
        """
    ]
    
    with engine.connect() as conn:
        for i, query in enumerate(analysis_queries):
            try:
                result = conn.execute(text(query))
                print(f"\\nüìä Analysis Query {i+1}:")
                for row in result:
                    print(f"   {row}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Query {i+1} failed (might need pg_stat_statements): {e}")

if __name__ == "__main__":
    print("üóÑÔ∏è RareSift Database Optimization")
    print("=" * 40)
    
    print("Creating performance indexes...")
    create_performance_indexes()
    
    print("\\nAnalyzing database performance...")
    analyze_database_performance()
    
    print("\\n‚úÖ Database optimization complete!")
'''
        
        db_script = self.project_root / "scripts" / "optimize-database.py"
        if not db_script.exists():
            db_script.write_text(db_optimization_script)
            self.optimizations_applied.append(f"Created database optimization script: {db_script}")
            return str(db_script)
        
        return ""
    
    def generate_optimization_report(self) -> str:
        """Generate optimization report"""
        report = []
        report.append("# RareSift Performance Optimization Report")
        report.append(f"Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Applied optimizations
        if self.optimizations_applied:
            report.append("## Optimizations Applied")
            for optimization in self.optimizations_applied:
                report.append(f"- ‚úÖ {optimization}")
            report.append("")
        
        # Recommendations
        if self.recommendations:
            report.append("## Performance Recommendations")
            for i, recommendation in enumerate(self.recommendations, 1):
                report.append(f"{i}. {recommendation}")
            report.append("")
        
        # Performance checklist
        report.append("## Performance Optimization Checklist")
        checklist_items = [
            "‚úÖ Database indexes for frequently queried fields",
            "‚úÖ Redis caching for search results and metadata",
            "‚úÖ Connection pooling for database connections",
            "‚úÖ Async/await patterns for I/O operations",
            "‚úÖ Gzip compression for HTTP responses",
            "‚úÖ Static asset optimization and CDN",
            "‚úÖ Docker image optimization with multi-stage builds",
            "‚úÖ Performance monitoring middleware",
            "‚úÖ Rate limiting to prevent abuse",
            "‚úÖ Proper error handling and logging"
        ]
        
        for item in checklist_items:
            report.append(f"   {item}")
        
        report.append("")
        report.append("## Next Steps")
        report.append("1. Run performance tests to establish baseline metrics")
        report.append("2. Implement recommended optimizations")
        report.append("3. Re-run performance tests to measure improvements")
        report.append("4. Monitor production performance continuously")
        report.append("5. Set up alerts for performance degradation")
        
        return "\\n".join(report)
    
    def run_optimizations(self):
        """Run all optimization tasks"""
        print("üöÄ RareSift Performance Optimization")
        print("=" * 50)
        
        # Analysis phase
        self.check_database_indexes()
        self.optimize_database_queries()
        self.optimize_docker_configuration()
        self.optimize_frontend_performance()
        self.optimize_backend_performance()
        self.optimize_nginx_configuration()
        
        # Implementation phase
        self.create_caching_configuration()
        self.create_performance_middleware()
        self.create_database_optimization_script()
        
        # Generate report
        report = self.generate_optimization_report()
        report_file = Path("performance-optimization-report.md")
        report_file.write_text(report)
        
        print(f"üìã Optimization report saved: {report_file}")
        print("")
        
        print("üìä Optimization Summary:")
        print(f"   ‚úÖ Applied: {len(self.optimizations_applied)} optimizations")
        print(f"   üí° Recommendations: {len(self.recommendations)} items")
        
        if self.optimizations_applied:
            print("\\nüéâ Performance optimizations applied successfully!")
        else:
            print("\\nüí° Review recommendations for manual implementation")
        
        return True

def main():
    """Main function"""
    optimizer = RareSiftOptimizer()
    return optimizer.run_optimizations()

if __name__ == "__main__":
    main()